import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from torch.nn.utils.rnn import pad_sequence
from torchtext.data.utils import get_tokenizer
from torchtext.vocab import build_vocab_from_iterator
import pandas as pd
from collections import Counter

# Cấu hình
MAX_LEN = 200
BATCH_SIZE = 64
EMBED_DIM = 128
HIDDEN_DIM = 256
EPOCHS = 5
LEARNING_RATE = 0.001
DEVICE = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# Đọc dữ liệu
df = pd.read_csv("IMDB Dataset.csv").sample(frac=1, random_state=42).head(10000)
texts = df['review'].tolist()
labels = df['sentiment'].apply(lambda x: 1 if x == 'positive' else 0).tolist()

# Tokenize & Vocab
tokenizer = get_tokenizer("basic_english")
tokenized_texts = [tokenizer(text) for text in texts]

def yield_tokens(data_iter):
    for tokens in data_iter:
        yield tokens

vocab = build_vocab_from_iterator(
    yield_tokens(tokenized_texts),
    specials=["<unk>", "<pad>"],
    max_tokens=10000
)
vocab.set_default_index(vocab["<unk>"])

# Encode và pad
def encode(tokens): return torch.tensor([vocab[token] for token in tokens], dtype=torch.long)
encoded = [encode(tokens[:MAX_LEN]) for tokens in tokenized_texts]
padded = pad_sequence(encoded, batch_first=True, padding_value=vocab["<pad>"])[:, :MAX_LEN]
labels_tensor = torch.tensor(labels, dtype=torch.float)

# Train/test split
x_train, y_train = padded[:5000], labels_tensor[:5000]
x_test, y_test = padded[5000:], labels_tensor[5000:]

train_loader = DataLoader(TensorDataset(x_train, y_train), batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(TensorDataset(x_test, y_test), batch_size=BATCH_SIZE)

# Mô hình RNN (GRU)
class SentimentRNN(nn.Module):
    def __init__(self):
        super().__init__()
        self.embedding = nn.Embedding(len(vocab), EMBED_DIM, padding_idx=vocab["<pad>"])
        self.gru = nn.GRU(EMBED_DIM, HIDDEN_DIM, batch_first=True)
        self.fc = nn.Linear(HIDDEN_DIM, 1)

    def forward(self, x):
        embedded = self.embedding(x)  # (batch, seq_len, embed_dim)
        _, hidden = self.gru(embedded)  # hidden: (1, batch, hidden_dim)
        out = self.fc(hidden.squeeze(0))  # (batch, 1)
        return torch.sigmoid(out).squeeze()

# Khởi tạo mô hình, loss, optimizer
model = SentimentRNN().to(DEVICE)
criterion = nn.BCELoss()
optimizer = optim.Adam(model.parameters(), lr=LEARNING_RATE)

# Huấn luyện
for epoch in range(EPOCHS):
    model.train()
    total_loss = 0
    for xb, yb in train_loader:
        xb, yb = xb.to(DEVICE), yb.to(DEVICE)
        optimizer.zero_grad()
        preds = model(xb)
        loss = criterion(preds, yb)
        loss.backward()
        optimizer.step()
        total_loss += loss.item()
    print(f"Epoch {epoch+1}/{EPOCHS} - Loss: {total_loss:.4f}")

# Đánh giá
model.eval()
correct = total = 0
with torch.no_grad():
    for xb, yb in test_loader:
        xb, yb = xb.to(DEVICE), yb.to(DEVICE)
        preds = (model(xb) > 0.5).float()
        correct += (preds == yb).sum().item()
        total += yb.size(0)

print(f"✅ Test Accuracy: {100 * correct / total:.2f}%")
