import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader, TensorDataset
from sklearn.datasets import fetch_california_housing
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from torch.utils.tensorboard import SummaryWriter
import numpy as np
import time
import os
from itertools import product

# Tập các cấu hình siêu tham số
param_grid = {
    'hidden_size': [64, 128],
    'num_layers': [3, 5],
    'learning_rate': [0.001, 0.0005],
}

EPOCHS = 100
BATCH_SIZE = 64
SEED = 42
NUM_RUNS = 5

# Đặt seed để kết quả có thể tái lập
torch.manual_seed(SEED)
np.random.seed(SEED)

# 1. Load dữ liệu
data = fetch_california_housing()
X = data.data
y = data.target.reshape(-1, 1)

# Kiểm tra dữ liệu thiếu
if np.isnan(X).sum() > 0 or np.isnan(y).sum() > 0:
    print("Dữ liệu có giá trị thiếu. Tiến hành loại bỏ...")
    mask = ~np.isnan(X).any(axis=1) & ~np.isnan(y).reshape(-1)
    X = X[mask]
    y = y[mask]
else:
    print("Không có dữ liệu thiếu.")

# 2. Chuẩn hóa dữ liệu
scaler_X = StandardScaler()
scaler_y = StandardScaler()
X = scaler_X.fit_transform(X)
y = scaler_y.fit_transform(y)

# 3. Chia train/test
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=SEED)
X_train = torch.tensor(X_train, dtype=torch.float32)
y_train = torch.tensor(y_train, dtype=torch.float32)
X_test = torch.tensor(X_test, dtype=torch.float32)
y_test = torch.tensor(y_test, dtype=torch.float32)

train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=BATCH_SIZE, shuffle=True)
test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=BATCH_SIZE)

# 4. Định nghĩa mô hình mạng MLP
class MLPRegressor(nn.Module):
    def __init__(self, input_size, hidden_size, output_size=1, num_layers=4):
        super(MLPRegressor, self).__init__()
        layers = [nn.Linear(input_size, hidden_size), nn.ReLU()]
        for _ in range(num_layers - 1):
            layers.append(nn.Linear(hidden_size, hidden_size))
            layers.append(nn.BatchNorm1d(hidden_size))
            layers.append(nn.ReLU())
            layers.append(nn.Dropout(0.3))
        layers.append(nn.Linear(hidden_size, output_size))
        self.model = nn.Sequential(*layers)

    def forward(self, x):
        return self.model(x)

# 5. Lặp qua các cấu hình siêu tham số
for config_id, (hidden_size, num_layers, lr) in enumerate(product(param_grid['hidden_size'], param_grid['num_layers'], param_grid['learning_rate'])):
    print(f"\n==== CONFIG {config_id+1}: hidden={hidden_size}, layers={num_layers}, lr={lr} ====")
    rmse_list = []
    real_rmse_list = []

    for run in range(NUM_RUNS):
        writer = SummaryWriter(log_dir=f"runs/config{config_id+1}_run{run+1}_{int(time.time())}")

        model = MLPRegressor(input_size=X.shape[1], hidden_size=hidden_size, num_layers=num_layers)
        criterion = nn.MSELoss()
        optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=30, gamma=0.5)

        for epoch in range(EPOCHS):
            model.train()
            train_loss = 0
            for xb, yb in train_loader:
                pred = model(xb)
                loss = criterion(pred, yb)
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()
                train_loss += loss.item() * xb.size(0)

            train_loss /= len(train_loader.dataset)
            writer.add_scalar("Loss/Train", train_loss, epoch)

            # Đánh giá trên test set
            model.eval()
            with torch.no_grad():
                test_loss = 0
                for xb, yb in test_loader:
                    pred = model(xb)
                    loss = criterion(pred, yb)
                    test_loss += loss.item() * xb.size(0)
                test_loss /= len(test_loader.dataset)
                writer.add_scalar("Loss/Test", test_loss, epoch)

            # Log learning rate
            current_lr = optimizer.param_groups[0]['lr']
            writer.add_scalar("LearningRate", current_lr, epoch)

            # Log histogram trọng số và gradient
            for name, param in model.named_parameters():
                if param.requires_grad:
                    writer.add_histogram(f"Weights/{name}", param, epoch)
                    if param.grad is not None:
                        writer.add_histogram(f"Gradients/{name}", param.grad, epoch)

            scheduler.step()

        # Tính RMSE
        model.eval()
        with torch.no_grad():
            pred = model(X_test)
            mse = nn.MSELoss()(pred, y_test)
            rmse = torch.sqrt(mse).item()
            rmse_list.append(rmse)

            # Tính RMSE thực tế (nghìn đô)
            y_pred_real = scaler_y.inverse_transform(pred.numpy())
            y_test_real = scaler_y.inverse_transform(y_test.numpy())
            real_rmse = np.sqrt(np.mean((y_pred_real - y_test_real) ** 2))
            real_rmse_list.append(real_rmse)
            print(f"Run {run + 1} RMSE: {rmse:.4f}, RMSE thực tế (nghìn đô): {real_rmse:.4f}")

        writer.close()

    mean_rmse = np.mean(rmse_list)
    std_rmse = np.std(rmse_list)
    mean_real_rmse = np.mean(real_rmse_list)
    std_real_rmse = np.std(real_rmse_list)
    print(f"CONFIG {config_id+1} --> RMSE chuẩn hóa: {mean_rmse:.4f} ± {std_rmse:.4f}, RMSE thực tế: {mean_real_rmse:.4f} ± {std_real_rmse:.4f}")
